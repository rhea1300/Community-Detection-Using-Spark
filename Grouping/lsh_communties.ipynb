{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document provides a starting point for the implementation of Locality Senstitive Hashing for further analysis. It is not currently ready to be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark setup & Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark setup\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.ml.feature import MinHashLSH, VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col, when, explode, lit, array_contains\n",
    "\n",
    "\n",
    "# libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "# show all rows with df.head\n",
    "pd.options.display.max_columns = None\n",
    "# remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "  .appName(\"DIS_project_5\") \\\n",
    "  .master(\"local[*]\") \\\n",
    "  .config(\"spark.driver.memory\", \"10G\") \\\n",
    "  .config(\"spa\\rk.driver.maxResultSize\", \"40g\") \\\n",
    "  .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \\\n",
    "  .getOrCreate()\n",
    "spark\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load community data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = \"../Community Detection/10K.json\"\n",
    "with open(json_file_path, 'r') as f:\n",
    "    community_data = json.load(f)\n",
    "\n",
    "community_rdd = sc.parallelize(community_data)\n",
    "print(community_rdd.take(3))\n",
    "print(community_rdd.count())\n",
    "print(type(community_rdd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Collect unique nodes and edges across all communities\n",
    "all_nodes = sorted({node for community in community_rdd.collect() for node in community['nodes']})\n",
    "all_edges = sorted({(edge['node1'], edge['node2']) for community in community_rdd.collect() for edge in community['edges']})\n",
    "\n",
    "# Step 2: Convert RDD to DataFrame with binary columns for each unique node and edge\n",
    "def create_binary_features(community):\n",
    "    features = {}\n",
    "    # Binary columns for nodes\n",
    "    for node in all_nodes:\n",
    "        features[f'node_{node}'] = 1 if node in community['nodes'] else 0\n",
    "    # Binary columns for edges\n",
    "    for edge in all_edges:\n",
    "        features[f'edge_{edge[0]}_{edge[1]}'] = 1 if edge in community['edges'] else 0\n",
    "    # Include the community_id for reference\n",
    "    features['community_id'] = community['community_id']\n",
    "    return Row(**features)\n",
    "\n",
    "# Apply create_binary_features to each community in the RDD\n",
    "binary_features_rdd = community_rdd.map(create_binary_features)\n",
    "\n",
    "# Convert the RDD to a DataFrame\n",
    "data_df = spark.createDataFrame(binary_features_rdd)\n",
    "\n",
    "\n",
    "#step 3; assemble all the node and edge columns into a single feature column using vector assembler\n",
    "feature_cols = [f\"node_{node}\" for node in all_nodes] + [f\"edge_{edge[0]}_{edge[1]}\" for edge in all_edges]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "vector_df = assembler.transform(data_df)\n",
    "print((vector_df.select([\"community_id\"]).take(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4; initialise MinHashLSH and fit the model\n",
    "minhash = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=5)\n",
    "model = minhash.fit(vector_df)\n",
    "\n",
    "# transform data and show hashes\n",
    "transformed_df = model.transform(vector_df)\n",
    "print(transformed_df.show())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test lsh model\n",
    "# key_test_community = {'community_id': 1, 'nodes': [1, 2, 9, 7, 3, 8], 'edges': [{'node1': 1, 'node2': 2, 'begintijd': 20240921180004, 'eindtijd': 20240921225419}, {'node1': 1, 'node2': 9, 'begintijd': 20240318051804, 'eindtijd': 20240318115006}, {'node1': 1, 'node2': 7, 'begintijd': 20240112134348, 'eindtijd': 20240112143558}, {'node1': 1, 'node2': 3, 'begintijd': 20240928030750, 'eindtijd': 20240928065938}, {'node1': 2, 'node2': 9, 'begintijd': 20241204014916, 'eindtijd': 20241204073145}, {'node1': 2, 'node2': 7, 'begintijd': 20240417225406, 'eindtijd': 20240418030243}, {'node1': 2, 'node2': 3, 'begintijd': 20240726103240, 'eindtijd': 20240726171152}, {'node1': 2, 'node2': 8, 'begintijd': 20241022091933, 'eindtijd': 20241022141128}, {'node1': 7, 'node2': 9, 'begintijd': 20240326143919, 'eindtijd': 20240326184538}, {'node1': 7, 'node2': 8, 'begintijd': 20240225113935, 'eindtijd': 20240225140514}, {'node1': 3, 'node2': 9, 'begintijd': 20240710155403, 'eindtijd': 20240710164441}, {'node1': 3, 'node2': 7, 'begintijd': 20240625050041, 'eindtijd': 20240625050327}, {'node1': 8, 'node2': 9, 'begintijd': 20240218112750, 'eindtijd': 20240218145533}]}\n",
    "\n",
    "first_key_vector = transformed_df.filter(transformed_df.community_id == 2).select(\"features\").first()[\"features\"]\n",
    "print(first_key_vector)\n",
    "# print(transformed_df.show(8))\n",
    "\n",
    "# remove vector from dataframe with id 2\n",
    "no_target_transformed_df = transformed_df.filter(transformed_df.community_id != 2)\n",
    "\n",
    "number_of_neighbours = 5\n",
    "# model.approxNearestNeighbors(no_target_transformed_df, first_key_vector, numNearestNeighbors=number_of_neighbours).show()\n",
    "model.approxNearestNeighbors(no_target_transformed_df, first_key_vector, numNearestNeighbors=number_of_neighbours).select(\"community_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nearest neighbours multiple at ones\n",
    "# print(model.approxSimilarityJoin(transformed_df, transformed_df, 0.6).show())\n",
    "print(transformed_df.show(1))\n",
    "print(model.approxSimilarityJoin(transformed_df, transformed_df.show(1), 100).select(\"datasetA.community_id\", \"datasetB.community_id\").show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find N nearest neighbors, by looping through each record in the data and finding the nearest neighbors for each record and saving them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test method, alternative data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_df1 = vector_df.take(2)\n",
    "print(f\"features in sparsevector of community 1: \\n{vec_df1[1].features}\")\n",
    "# sparse_vec_df1 = Vectors.sparse(vec_df1)\n",
    "# print(sparse_vec_df1)\n",
    "\n",
    "formatted_data = vector_df.select(\"community_id\", \"features\").rdd.map(lambda row: (row.community_id, row.features))\n",
    "print(formatted_data.collect())\n",
    "\n",
    "# Row(nodes, edges, community_id=1, features=SparseVector(1722, {39: 1.0, 55: 1.0, 60: 1.0, 61: 1.0, 82: 1.0, 142: 1.0, 195: 1.0, 223: 1.0, 296: 1.0, 320: 1.0, 322: 1.0, 325: 1.0, 330: 1.0, 332: 1.0, 351: 1.0, 375: 1.0, 405: 1.0, 407: 1.0, 475: 1.0, 512: 1.0, 528: 1.0, 541: 1.0, 558: 1.0, 579: 1.0, 582: 1.0, 614: 1.0, 664: 1.0, 702: 1.0, 727: 1.0, 760: 1.0, 772: 1.0, 780: 1.0, 804: 1.0, 846: 1.0, 847: 1.0})), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# define OG community dataset\n",
    "temp_data = [(0, Vectors.sparse(6, [0, 1, 2], [1.0, 1.0, 1.0])),\n",
    "             (1, Vectors.sparse(6, [2, 3, 4], [1.0, 1.0, 1.0])),\n",
    "             (2, Vectors.sparse(6, [0, 2, 4], [1.0, 1.0, 1.0]))]\n",
    "# print(f\"temp_data:\\n {temp_data}\")\n",
    "\n",
    "\n",
    "temp_data2 = formatted_data\n",
    "# print(f\"temp_data2: \\n {temp_data2}\")\n",
    "\n",
    "\n",
    "# create dataframe\n",
    "temp_df = spark.createDataFrame(temp_data, [\"id\", \"features\"])\n",
    "# print(f\"temp_df:\\n {temp_df.collect()}\")\n",
    "\n",
    "temp_df2 = spark.createDataFrame(temp_data2, [\"id\", \"features\"])\n",
    "# print(f\"temp_df2:\\n {temp_df2.collect()}\")\n",
    "\n",
    "\n",
    "# create model on dataset\n",
    "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=5)\n",
    "mh.setSeed(42)\n",
    "model = mh.fit(temp_df)\n",
    "# print(f\"model:\\n {model}\")\n",
    "\n",
    "model2 = mh.fit(temp_df2)\n",
    "# print(f\"model2:\\n {model2}\")\n",
    "\n",
    "\n",
    "# example apporx neighbour\n",
    "# key = Vectors.sparse(6, [0, 1], [1.0, 1.0])\n",
    "# model.approxNearestNeighbors(temp_df, key, 3).collect()\n",
    "\n",
    "\n",
    "# getting cross reverence for approximty for all\n",
    "result = model.approxSimilarityJoin(temp_df, temp_df, threshold=float('inf')).filter(\"datasetA.id != datasetB.id\")\n",
    "results_df = result.select(\n",
    "  col(\"datasetA.id\").alias(\"idA\"),\n",
    "  col(\"datasetB.id\").alias(\"idB\"),\n",
    "  col(\"distCol\").alias(\"JaccardDistance\")\n",
    "  )\n",
    "print(f\"count of temp_df: {temp_df.count()}\")\n",
    "print(f\"count of results_df: {results_df.count()}\")\n",
    "results_df.show()\n",
    "\n",
    "result2 = model2.approxSimilarityJoin(temp_df2, temp_df2, threshold=float('inf'))\n",
    "results_df2 = result2.select(\n",
    "  col(\"datasetA.id\").alias(\"idA\"),\n",
    "  col(\"datasetB.id\").alias(\"idB\"),\n",
    "  col(\"distCol\").alias(\"JaccardDistance\")\n",
    "  )\n",
    "print(f\"count of temp_df2: {temp_df2.count()}\")\n",
    "print(f\"count of results_df2: {results_df2.count()}\")\n",
    "results_df2.show()\n",
    "\n",
    "# print results\n",
    "# print(f\"result type: {type(results_df)}\")\n",
    "# print(f\"result2 type: {type(results_df2)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_df2.rdd.getNumPartitions())\n",
    "tempyNu = sc.parallelize(results_df2.rdd.collect(), numSlices=100)\n",
    "# results_df2.repartition(100).rdd.getNumPartitions()\n",
    "print(tempyNu.rdd.getNumPartitions())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
