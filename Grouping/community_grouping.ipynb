{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file takes as input the detected communities from the Louvain algorithm and outputs the groups of communities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark setup & libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark setup\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import MinHashLSH, VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col, when, explode, lit, array_contains\n",
    "# from itertools import groupby\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number, ceil, col, udf\n",
    "import math\n",
    "\n",
    "import networkx as nx\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from collections import defaultdict\n",
    "\n",
    "# libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "# show all rows with df.head\n",
    "pd.options.display.max_columns = None\n",
    "# remove warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create spark session + load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "  .appName(\"DIS_project_5\") \\\n",
    "  .master(\"local[*]\") \\\n",
    "  .config(\"spark.driver.memory\", \"10G\") \\\n",
    "  .config(\"spa\\rk.driver.maxResultSize\", \"40g\") \\\n",
    "  .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") \\\n",
    "  .getOrCreate()\n",
    "spark\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = \"../Community Detection/20Knode_20edge.json\"\n",
    "with open(json_file_path, 'r') as f:\n",
    "    community_data = json.load(f)\n",
    "\n",
    "community_rdd = sc.parallelize(community_data)\n",
    "print(community_rdd.take(3))\n",
    "print(community_rdd.count())\n",
    "print(type(community_rdd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn rdd into dataframe and add columns for count of nodes and edges\n",
    "df_community = spark.createDataFrame(community_rdd)\n",
    "df_community.show()\n",
    "# Add columns for node size and edge size\n",
    "df_with_sizes = df_community \\\n",
    "    .withColumn(\"node_size\", F.size(\"nodes\")) \\\n",
    "    .withColumn(\"edge_size\", F.size(\"edges\")) \\\n",
    "\n",
    "df_with_sizes.show()\n",
    "\n",
    "# group communities by number of nodes\n",
    "# Group by node size\n",
    "node_size_groups = df_with_sizes.groupBy(\"node_size\").count()\n",
    "node_size_groups.show()\n",
    "\n",
    "\n",
    "# group communities by number of edges\n",
    "# Group by edge size\n",
    "edge_size_groups = df_with_sizes.groupBy(\"edge_size\").count()\n",
    "edge_size_groups.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make N groups by size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group communities by number of nodes and edges\n",
    "def group_by_size(df, num_groups, column_name):\n",
    "  # sort the data by the column name\n",
    "  sorted_df = df.sort(column_name)\n",
    "\n",
    "  # calculate the size of each group\n",
    "  group_size = math.ceil(df.count() / num_groups)\n",
    "  \n",
    "  # add a row number column to the dataframe\n",
    "  df_with_row_number = df.withColumn(\"row_number\", row_number().over(Window.orderBy(column_name)))\n",
    "\n",
    "  # use row number to create groups\n",
    "  df_groups = df_with_row_number.withColumn(\"group\", ceil(col(\"row_number\") / group_size)).drop(\"row_number\")\n",
    "\n",
    "  return df_groups\n",
    "\n",
    "\n",
    "result = group_by_size(df_with_sizes, 3, 'node_size')\n",
    "result.show(55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Unique Pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make unique pairs of communities in same group\n",
    "def create_pairs_grouped_communities(df_groups):\n",
    "  df_pairs = (\n",
    "    df_groups.alias(\"df1\")\n",
    "    .join(df_groups.alias(\"df2\"), (F.col(\"df1.group\") == F.col(\"df2.group\")) & (F.col(\"df1.community_id\") < F.col(\"df2.community_id\")))\n",
    "    # .withColumn(\"pairs\", (F.col(\"df1.community_id\") + F.lit(\", \") + F.col(\"df2.community_id\")))\n",
    "    .select(\n",
    "      F.array(F.col(\"df1.community_id\"), F.col(\"df2.community_id\")).alias(\"pairs\"),\n",
    "      F.col(\"df1.group\").alias(\"group\"),\n",
    "      F.col(\"df1.community_id\").alias(\"community_id_1\"),\n",
    "      F.col(\"df2.community_id\").alias(\"community_id_2\"),\n",
    "      F.col(\"df1.nodes\").alias(\"nodes_1\"),\n",
    "      F.col(\"df2.nodes\").alias(\"nodes_2\"),\n",
    "      F.col(\"df1.edges\").alias(\"edges_1\"),\n",
    "      F.col(\"df2.edges\").alias(\"edges_2\"),\n",
    "    )\n",
    "  )\n",
    "\n",
    "  return df_pairs\n",
    "\n",
    "result_group_communities = create_pairs_grouped_communities(result)\n",
    "result_group_communities.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn rdd of communities into dataframe of pairs of communities of similiar node or edge size\n",
    "def rdd_community_to_dataframe_paired_community_by_column(rdd_communities, number_groups_of_communities_wanted, column_name):\n",
    "  # turn rdd into dataframe and add columns for count of nodes and edges\n",
    "  df_community = spark.createDataFrame(rdd_communities)\n",
    "  df_with_sizes = df_community \\\n",
    "    .withColumn(\"node_size\", F.size(\"nodes\")) \\\n",
    "    .withColumn(\"edge_size\", F.size(\"edges\")) \\\n",
    "\n",
    "  # group communities by number of nodes and edges\n",
    "  df_communities_groups = group_by_size(df_with_sizes, number_groups_of_communities_wanted, column_name)\n",
    "\n",
    "  # make unique pairs of communities in same group\n",
    "  result_grouped_communities = create_pairs_grouped_communities(df_communities_groups)\n",
    "\n",
    "  # from this dataframe you can see which communities are grouped together, so all the pairs as a seperate column\n",
    "\n",
    "  return result_grouped_communities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Dataframe of grouped communities\n",
    "grouped_communities=rdd_community_to_dataframe_paired_community_by_column(community_rdd, 3, 'node_size')\n",
    "\n",
    "#Converting it to RDD\n",
    "pairs_rdd = grouped_communities.rdd\n",
    "type(pairs_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Temporal NetSimile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###TemporalNetSimile defintion\n",
    "\"\"\"\n",
    "Temporalnetsimile.py\n",
    "--------------\n",
    "\n",
    "Graph distance based on:\n",
    "Berlingerio, M., Koutra, D., Eliassi-Rad, T. & Faloutsos, C. NetSimile: A Scalable Approach to Size-Independent Network Similarity. arXiv (2012)\n",
    "\n",
    "Extended upon netsimile.py, acreditted to:\n",
    "\n",
    "author: Alex Gates\n",
    "email: ajgates42@gmail.com (optional)\n",
    "Submitted as part of the 2019 NetSI Collabathon.\n",
    "https://netrd.readthedocs.io/en/latest/_modules/netrd/distance/netsimile.html\n",
    "\n",
    "We wish to note that extensions upon the initial netsimile code is preceded by a ###comment\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import canberra\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "from netrd.distance.base import BaseDistance\n",
    "from netrd.utilities import undirected, unweighted\n",
    "\n",
    "###Needed to convert time later\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class TemporalNetSimile(BaseDistance):\n",
    "    \"\"\"Compares node signature distributions.\"\"\"\n",
    "\n",
    "\n",
    "    @undirected\n",
    "    @unweighted\n",
    "    def dist(self, G1, G2):\n",
    "        \"\"\"A scalable approach to network similarity.\n",
    "\n",
    "        A network similarity measure based on node signature distributions.\n",
    "        \n",
    "        The results dictionary includes the underlying feature matrices in\n",
    "        `'feature_matrices'` and the underlying signature vectors in\n",
    "        `'signature_vectors'`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        G1, G2 (nx.Graph)\n",
    "            two undirected networkx graphs to be compared.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        dist (float)\n",
    "            the distance between `G1` and `G2`.\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "\n",
    "        .. [1] Michele Berlingerio, Danai Koutra, Tina Eliassi-Rad,\n",
    "               Christos Faloutsos: NetSimile: A Scalable Approach to\n",
    "               Size-Independent Network Similarity. CoRR abs/1209.2684\n",
    "               (2012)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # find the graph node feature matrices\n",
    "        G1_node_features = feature_extraction(G1)\n",
    "        G2_node_features = feature_extraction(G2)\n",
    "\n",
    "        # get the graph signature vectors\n",
    "        G1_signature = graph_signature(G1_node_features)\n",
    "        G2_signature = graph_signature(G2_node_features)\n",
    "\n",
    "        # the final distance is the absolute canberra distance\n",
    "        dist = abs(canberra(G1_signature, G2_signature))\n",
    "\n",
    "        ###Remove for quicker comp.\n",
    "        ###self.results['feature_matrices'] = G1_node_features, G2_node_features\n",
    "        ###self.results['signature_vectors'] = G1_signature, G2_signature\n",
    "        \n",
    "        self.results['dist'] = dist\n",
    "\n",
    "        return dist\n",
    "\n",
    "\n",
    "\n",
    "def feature_extraction(G):\n",
    "    \"\"\"Node feature extraction.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    G (nx.Graph): a networkx graph.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    node_features (float): the Nx7 matrix of node features.\"\"\"\n",
    "\n",
    "    # necessary data structures\n",
    "    ###FEATURE NUMBER\n",
    "    node_features = np.zeros(shape=(G.number_of_nodes(), 10)) \n",
    "    node_list = sorted(G.nodes())\n",
    "    node_degree_dict = dict(G.degree())\n",
    "    node_clustering_dict = dict(nx.clustering(G))\n",
    "    egonets = {n: nx.ego_graph(G, n) for n in node_list}\n",
    "\n",
    "    # node degrees\n",
    "    degs = [node_degree_dict[n] for n in node_list]\n",
    "\n",
    "    # clustering coefficient\n",
    "    clusts = [node_clustering_dict[n] for n in node_list]\n",
    "\n",
    "    # average degree of neighborhood\n",
    "    neighbor_degs = [\n",
    "        np.mean([node_degree_dict[m] for m in egonets[n].nodes if m != n])\n",
    "        if node_degree_dict[n] > 0\n",
    "        else 0\n",
    "        for n in node_list\n",
    "    ]\n",
    "\n",
    "    # average clustering coefficient of neighborhood\n",
    "    neighbor_clusts = [\n",
    "        np.mean([node_clustering_dict[m] for m in egonets[n].nodes if m != n])\n",
    "        if node_degree_dict[n] > 0\n",
    "        else 0\n",
    "        for n in node_list\n",
    "    ]\n",
    "\n",
    "    # number of edges in the neighborhood\n",
    "    neighbor_edges = [\n",
    "        egonets[n].number_of_edges() if node_degree_dict[n] > 0 else 0\n",
    "        for n in node_list\n",
    "    ]\n",
    "\n",
    "    # number of outgoing edges from the neighborhood\n",
    "    # the sum of neighborhood degrees = 2*(internal edges) + external edges\n",
    "    # node_features[:,5] = node_features[:,0] * node_features[:,2] - 2*node_features[:,4]\n",
    "    neighbor_outgoing_edges = [\n",
    "        len(\n",
    "            [\n",
    "                edge\n",
    "                for edge in set.union(*[set(G.edges(j)) for j in egonets[i].nodes])\n",
    "                if not egonets[i].has_edge(*edge)\n",
    "            ]\n",
    "        )\n",
    "        for i in node_list\n",
    "    ]\n",
    "\n",
    "    # number of neighbors of neighbors (not in neighborhood)\n",
    "    neighbors_of_neighbors = [\n",
    "        len(\n",
    "            set([p for m in G.neighbors(n) for p in G.neighbors(m)])\n",
    "            - set(G.neighbors(n))\n",
    "            - set([n])\n",
    "        )\n",
    "        if node_degree_dict[n] > 0\n",
    "        else 0\n",
    "        for n in node_list\n",
    "    ]\n",
    "\n",
    "    ###Temporal features on the edges\n",
    "\n",
    "    ###start time, average start time of the edges of a single node\n",
    "    ###Loop over all nodes, look at the starttime data from the edges, convert the time to time format, calculate mean start time for each node\n",
    "    startTime = []\n",
    "    for n in node_list:\n",
    "        start_times = []\n",
    "        for u, v, data in G.edges(n, data=True):\n",
    "            # Convert begintijd to a datetime object\n",
    "            start_time_str = str(data['begintijd'])\n",
    "            start_time = datetime.strptime(start_time_str, '%Y%m%d%H%M%S')\n",
    "            start_times.append(start_time.timestamp())  # Store as timestamp (seconds since epoch)\n",
    "\n",
    "        startTime.append(np.mean(start_times) if start_times else 0)\n",
    "\n",
    "    ###end time, average end time of the edges of a single node\n",
    "    ###Loop over all nodes, look at the endtime data from the edges, convert the time to time format, calculate mean end time for each node\n",
    "    endTime = []\n",
    "    for n in node_list:\n",
    "        end_times = []\n",
    "        for u, v, data in G.edges(n, data=True):\n",
    "            # Convert begintijd to a datetime object\n",
    "            end_time_str = str(data['eindtijd'])\n",
    "            end_time = datetime.strptime(end_time_str, '%Y%m%d%H%M%S')\n",
    "            end_times.append(end_time.timestamp())  # Store as timestamp (seconds since epoch)\n",
    "\n",
    "        endTime.append(np.mean(end_times) if end_times else 0)\n",
    "    \n",
    "    ###duration, average duration of the edges of a single node\n",
    "    duration = []\n",
    "    for n in node_list:\n",
    "        duration.append(endTime[node_list.index(n)] - startTime[node_list.index(n)])\n",
    "    ### end change\n",
    "\n",
    "    # assembling the features\n",
    "    node_features[:, 0] = degs\n",
    "    node_features[:, 1] = clusts\n",
    "    node_features[:, 2] = neighbor_degs\n",
    "    node_features[:, 3] = neighbor_clusts\n",
    "    node_features[:, 4] = neighbor_edges\n",
    "    node_features[:, 5] = neighbor_outgoing_edges\n",
    "    node_features[:, 6] = neighbors_of_neighbors\n",
    "\n",
    "    ###Time features\n",
    "    node_features[:, 7] = startTime\n",
    "    node_features[:, 8] = endTime\n",
    "    node_features[:, 9] = duration\n",
    "\n",
    "    return np.nan_to_num(node_features)\n",
    "\n",
    "\n",
    "def graph_signature(node_features):\n",
    "    ###FEATURE NUMBER\n",
    "    signature_vec = np.zeros(10 * 5)\n",
    "\n",
    "    # for each of the features\n",
    "    ### FEATURE NUMBER\n",
    "    for k in range(10):\n",
    "        # find the mean\n",
    "        signature_vec[k * 5] = node_features[:, k].mean()\n",
    "        # find the median\n",
    "        signature_vec[k * 5 + 1] = np.median(node_features[:, k])\n",
    "        # find the std\n",
    "        signature_vec[k * 5 + 2] = node_features[:, k].std()\n",
    "        # find the skew\n",
    "        signature_vec[k * 5 + 3] = skew(node_features[:, k])\n",
    "        # find the kurtosis\n",
    "        signature_vec[k * 5 + 4] = kurtosis(node_features[:, k])\n",
    "\n",
    "    return signature_vec\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# sample usage\n",
    ">>>G1 = nx.karate_club_graph()\n",
    ">>>G2 = nx.krackhardt_kite_graph()\n",
    "\n",
    ">>>test = TemporalNetSimile()\n",
    ">>>print(test.dist(G1, G2))\n",
    "11.45\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data to graph, run Temporal NetSimile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function that takes data from two communities and output a Distance\n",
    "def Compare(Community1, Community2):\n",
    "    #Convert into networkX graph to use as input for TemportalNetSimile\n",
    "    #Load Graphs\n",
    "    Graph1 = nx.Graph()\n",
    "    Graph1.add_nodes_from(Community1[\"nodes\"])\n",
    "\n",
    "    # Add edges with timestamps as edge attributes\n",
    "    for edge in Community1[\"edges\"]:\n",
    "        Graph1.add_edge(edge[\"node1\"], edge[\"node2\"], begintijd=edge[\"begintijd\"], eindtijd=edge[\"eindtijd\"])\n",
    "\n",
    "    Graph2=nx.Graph()\n",
    "    Graph2.add_nodes_from(Community2[\"nodes\"])\n",
    "    for edge in Community2[\"edges\"]:\n",
    "        Graph2.add_edge(edge[\"node1\"], edge[\"node2\"], begintijd=edge[\"begintijd\"], eindtijd=edge[\"eindtijd\"])\n",
    "\n",
    "    testTime=TemporalNetSimile()\n",
    "    Distance = testTime.dist(Graph1, Graph2)\n",
    "    return(Distance)\n",
    "\n",
    "#print(Compare(Community1,Community2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute similarity for every pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define a function to compute similarity\n",
    "def compute_similarity(row):\n",
    "    # Extract community data from the row\n",
    "    community1 = {\n",
    "        'community_id': row['community_id_1'],\n",
    "        'nodes': row['nodes_1'],\n",
    "        'edges': row['edges_1']\n",
    "    }\n",
    "    community2 = {\n",
    "        'community_id': row['community_id_2'],\n",
    "        'nodes': row['nodes_2'],\n",
    "        'edges': row['edges_2']\n",
    "    }\n",
    "    group = row['group']\n",
    "\n",
    "    # Compute the distance using the Compare function\n",
    "    distance = Compare(community1, community2)\n",
    "\n",
    "    # Return a tuple: (group, community_id1, community_id2, distance)\n",
    "    return (group, community1['community_id'], community2['community_id'], distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Apply the function to each row in the RDD\n",
    "similarities_rdd = pairs_rdd.map(compute_similarity)\n",
    "\n",
    "# Step 4: Collect the results\n",
    "similarities = similarities_rdd.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organize by group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize similarities by group\n",
    "group_similarities = defaultdict(list)\n",
    "for group, community_id1, community_id2, distance in similarities:\n",
    "    group_similarities[group].append((community_id1, community_id2, distance))\n",
    "\n",
    "print(group_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign communities to groups based on a threshold using average linking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "\n",
    "for group, group_sims in group_similarities.items():\n",
    "    # Get the set of unique community IDs in the group\n",
    "    community_ids = set()\n",
    "    for community_id1, community_id2, _ in group_sims:\n",
    "        community_ids.add(community_id1)\n",
    "        community_ids.add(community_id2)\n",
    "    community_ids = sorted(community_ids)\n",
    "    id_to_index = {community_id: idx for idx, community_id in enumerate(community_ids)}\n",
    "    N = len(community_ids)\n",
    "    \n",
    "    # Initialize a full distance matrix with zeros or a large number for missing values\n",
    "    distance_matrix = np.full((N, N), np.inf)\n",
    "    np.fill_diagonal(distance_matrix, 0)  # Distance to self is zero\n",
    "\n",
    "    # Fill in the distances\n",
    "    for community_id1, community_id2, distance in group_sims:\n",
    "        i = id_to_index[community_id1]\n",
    "        j = id_to_index[community_id2]\n",
    "        distance_matrix[i, j] = distance\n",
    "        distance_matrix[j, i] = distance  # Symmetric matrix\n",
    "    \n",
    "    # Convert to condensed distance matrix required by linkage function\n",
    "    condensed_dists = squareform(distance_matrix)\n",
    "\n",
    "    # Apply hierarchical clustering\n",
    "    Z = linkage(condensed_dists, method='average')  # You can try other methods like 'single', 'complete', etc.\n",
    "\n",
    "    # Choose a threshold or number of clusters\n",
    "    \n",
    "    distances = [distance for _, _, distance in group_sims]\n",
    "    mean_distance = np.mean(distances)\n",
    "    median_distance = np.median(distances)\n",
    "    std_distance = np.std(distances)\n",
    "    print(f\"Group {group} - Mean: {mean_distance}, Median: {median_distance}, Std: {std_distance}\")\n",
    "\n",
    "    threshold = mean_distance - std_distance\n",
    "    print(f\"Group {group} - Threshold: {threshold}\")\n",
    "    # Adjust based on your analysis for this group\n",
    "    # Option 2: Specify the number of clusters\n",
    "    # num_clusters = 3  # Adjust based on desired number of clusters\n",
    "\n",
    "    # Get cluster labels\n",
    "    cluster_labels = fcluster(Z, t=threshold, criterion='distance')\n",
    "    # cluster_labels = fcluster(Z, t=num_clusters, criterion='maxclust')\n",
    "\n",
    "    # Group communities by cluster labels\n",
    "    clusters = defaultdict(list)\n",
    "    for community_id, cluster_label in zip(community_ids, cluster_labels):\n",
    "        clusters[cluster_label].append(community_id)\n",
    "    \n",
    "    # Output the clusters for this group\n",
    "    print(f\"\\nGroup {group} Clustering Results:\")\n",
    "    for cluster_label, community_list in clusters.items():\n",
    "        print(f\"  Cluster {cluster_label}: {sorted(community_list)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the distances within groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for group, group_sims in group_similarities.items():\n",
    "    # Extract distances\n",
    "    distances = [distance for _, _, distance in group_sims]\n",
    "    \n",
    "    # Plot histogram of distances for the group\n",
    "    plt.figure()\n",
    "    plt.hist(distances, bins=20)\n",
    "    plt.title(f'Group {group} Distance Distribution')\n",
    "    plt.xlabel('Distance')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate statistical measures\n",
    "    mean_distance = np.mean(distances)\n",
    "    median_distance = np.median(distances)\n",
    "    std_distance = np.std(distances)\n",
    "    print(f\"Group {group} - Mean: {mean_distance}, Median: {median_distance}, Std: {std_distance}\")\n",
    "    \n",
    "    # Set threshold based on analysis\n",
    "    # For example, threshold = median_distance + std_distance\n",
    "    threshold = median_distance + std_distance\n",
    "    # Proceed with clustering as in previous step using this threshold\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
