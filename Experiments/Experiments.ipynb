{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is used in the justification of the grouping method, as well as in the experiments for pairing type, threshold values, and linkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libararies and start Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import json\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "import seaborn as sns\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from netrd.distance import NetSimile\n",
    "from netrd.distance.base import BaseDistance\n",
    "from netrd.utilities import undirected, unweighted\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import ceil, col, row_number, udf\n",
    "from scipy.cluster.hierarchy import dendrogram, fcluster, linkage\n",
    "from scipy.spatial.distance import canberra, pdist, squareform\n",
    "from scipy.stats import gaussian_kde, kurtosis, skew\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"CommunityRDD\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Temporal NetSimile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###TemporalNetSimile defintion\n",
    "\"\"\"\n",
    "Temporalnetsimile.py\n",
    "--------------\n",
    "\n",
    "Graph distance based on:\n",
    "Berlingerio, M., Koutra, D., Eliassi-Rad, T. & Faloutsos, C. NetSimile: A Scalable Approach to Size-Independent Network Similarity. arXiv (2012)\n",
    "\n",
    "Extended upon netsimile.py, acreditted to:\n",
    "\n",
    "author: Alex Gates\n",
    "email: ajgates42@gmail.com (optional)\n",
    "Submitted as part of the 2019 NetSI Collabathon.\n",
    "https://netrd.readthedocs.io/en/latest/_modules/netrd/distance/netsimile.html\n",
    "\n",
    "We wish to note that extensions upon the initial netsimile code is preceded by a ###comment\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "class TemporalNetSimile(BaseDistance):\n",
    "    \"\"\"Compares node signature distributions.\"\"\"\n",
    "\n",
    "\n",
    "    @undirected\n",
    "    @unweighted\n",
    "    def dist(self, G1, G2):\n",
    "        \"\"\"A scalable approach to network similarity.\n",
    "\n",
    "        A network similarity measure based on node signature distributions.\n",
    "        \n",
    "        The results dictionary includes the underlying feature matrices in\n",
    "        `'feature_matrices'` and the underlying signature vectors in\n",
    "        `'signature_vectors'`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        G1, G2 (nx.Graph)\n",
    "            two undirected networkx graphs to be compared.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        dist (float)\n",
    "            the distance between `G1` and `G2`.\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "\n",
    "        .. [1] Michele Berlingerio, Danai Koutra, Tina Eliassi-Rad,\n",
    "               Christos Faloutsos: NetSimile: A Scalable Approach to\n",
    "               Size-Independent Network Similarity. CoRR abs/1209.2684\n",
    "               (2012)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # find the graph node feature matrices\n",
    "        G1_node_features = feature_extraction(G1)\n",
    "        G2_node_features = feature_extraction(G2)\n",
    "\n",
    "        # get the graph signature vectors\n",
    "        G1_signature = graph_signature(G1_node_features)\n",
    "        G2_signature = graph_signature(G2_node_features)\n",
    "\n",
    "        # the final distance is the absolute canberra distance\n",
    "        dist = abs(canberra(G1_signature, G2_signature))\n",
    "\n",
    "        ###Remove for quicker comp.\n",
    "        ###self.results['feature_matrices'] = G1_node_features, G2_node_features\n",
    "        ###self.results['signature_vectors'] = G1_signature, G2_signature\n",
    "        \n",
    "        self.results['dist'] = dist\n",
    "\n",
    "        return dist\n",
    "\n",
    "\n",
    "\n",
    "def feature_extraction(G):\n",
    "    \"\"\"Node feature extraction.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    G (nx.Graph): a networkx graph.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    node_features (float): the Nx7 matrix of node features.\"\"\"\n",
    "\n",
    "    # necessary data structures\n",
    "    ###FEATURE NUMBER\n",
    "    node_features = np.zeros(shape=(G.number_of_nodes(), 10)) \n",
    "    node_list = sorted(G.nodes())\n",
    "    node_degree_dict = dict(G.degree())\n",
    "    node_clustering_dict = dict(nx.clustering(G))\n",
    "    egonets = {n: nx.ego_graph(G, n) for n in node_list}\n",
    "\n",
    "    # node degrees\n",
    "    degs = [node_degree_dict[n] for n in node_list]\n",
    "\n",
    "    # clustering coefficient\n",
    "    clusts = [node_clustering_dict[n] for n in node_list]\n",
    "\n",
    "    # average degree of neighborhood\n",
    "    neighbor_degs = [\n",
    "        np.mean([node_degree_dict[m] for m in egonets[n].nodes if m != n])\n",
    "        if node_degree_dict[n] > 0\n",
    "        else 0\n",
    "        for n in node_list\n",
    "    ]\n",
    "\n",
    "    # average clustering coefficient of neighborhood\n",
    "    neighbor_clusts = [\n",
    "        np.mean([node_clustering_dict[m] for m in egonets[n].nodes if m != n])\n",
    "        if node_degree_dict[n] > 0\n",
    "        else 0\n",
    "        for n in node_list\n",
    "    ]\n",
    "\n",
    "    # number of edges in the neighborhood\n",
    "    neighbor_edges = [\n",
    "        egonets[n].number_of_edges() if node_degree_dict[n] > 0 else 0\n",
    "        for n in node_list\n",
    "    ]\n",
    "\n",
    "    # number of outgoing edges from the neighborhood\n",
    "    # the sum of neighborhood degrees = 2*(internal edges) + external edges\n",
    "    # node_features[:,5] = node_features[:,0] * node_features[:,2] - 2*node_features[:,4]\n",
    "    neighbor_outgoing_edges = [\n",
    "        len(\n",
    "            [\n",
    "                edge\n",
    "                for edge in set.union(*[set(G.edges(j)) for j in egonets[i].nodes])\n",
    "                if not egonets[i].has_edge(*edge)\n",
    "            ]\n",
    "        )\n",
    "        for i in node_list\n",
    "    ]\n",
    "\n",
    "    # number of neighbors of neighbors (not in neighborhood)\n",
    "    neighbors_of_neighbors = [\n",
    "        len(\n",
    "            set([p for m in G.neighbors(n) for p in G.neighbors(m)])\n",
    "            - set(G.neighbors(n))\n",
    "            - set([n])\n",
    "        )\n",
    "        if node_degree_dict[n] > 0\n",
    "        else 0\n",
    "        for n in node_list\n",
    "    ]\n",
    "\n",
    "    ###Temporal features on the edges\n",
    "\n",
    "    ###start time, average start time of the edges of a single node\n",
    "    ###Loop over all nodes, look at the starttime data from the edges, convert the time to time format, calculate mean start time for each node\n",
    "    startTime = []\n",
    "    for n in node_list:\n",
    "        start_times = []\n",
    "        for u, v, data in G.edges(n, data=True):\n",
    "            # Convert begintijd to a datetime object\n",
    "            start_time_str = str(data['begintijd'])\n",
    "            start_time = datetime.strptime(start_time_str, '%Y%m%d%H%M%S')\n",
    "            start_times.append(start_time.timestamp())  # Store as timestamp (seconds since epoch)\n",
    "\n",
    "        startTime.append(np.mean(start_times) if start_times else 0)\n",
    "\n",
    "    ###end time, average end time of the edges of a single node\n",
    "    ###Loop over all nodes, look at the endtime data from the edges, convert the time to time format, calculate mean end time for each node\n",
    "    endTime = []\n",
    "    for n in node_list:\n",
    "        end_times = []\n",
    "        for u, v, data in G.edges(n, data=True):\n",
    "            # Convert begintijd to a datetime object\n",
    "            end_time_str = str(data['eindtijd'])\n",
    "            end_time = datetime.strptime(end_time_str, '%Y%m%d%H%M%S')\n",
    "            end_times.append(end_time.timestamp())  # Store as timestamp (seconds since epoch)\n",
    "\n",
    "        endTime.append(np.mean(end_times) if end_times else 0)\n",
    "    \n",
    "    ###duration, average duration of the edges of a single node\n",
    "    duration = []\n",
    "    for n in node_list:\n",
    "        duration.append(endTime[node_list.index(n)] - startTime[node_list.index(n)])\n",
    "    ### end change\n",
    "\n",
    "    # assembling the features\n",
    "    node_features[:, 0] = degs\n",
    "    node_features[:, 1] = clusts\n",
    "    node_features[:, 2] = neighbor_degs\n",
    "    node_features[:, 3] = neighbor_clusts\n",
    "    node_features[:, 4] = neighbor_edges\n",
    "    node_features[:, 5] = neighbor_outgoing_edges\n",
    "    node_features[:, 6] = neighbors_of_neighbors\n",
    "\n",
    "    ###Time features\n",
    "    node_features[:, 7] = startTime\n",
    "    node_features[:, 8] = endTime\n",
    "    node_features[:, 9] = duration\n",
    "\n",
    "    return np.nan_to_num(node_features)\n",
    "\n",
    "\n",
    "def graph_signature(node_features):\n",
    "    ###FEATURE NUMBER\n",
    "    signature_vec = np.zeros(10 * 5)\n",
    "\n",
    "    # for each of the features\n",
    "    ### FEATURE NUMBER\n",
    "    for k in range(10):\n",
    "        # find the mean\n",
    "        signature_vec[k * 5] = node_features[:, k].mean()\n",
    "        # find the median\n",
    "        signature_vec[k * 5 + 1] = np.median(node_features[:, k])\n",
    "        # find the std\n",
    "        signature_vec[k * 5 + 2] = node_features[:, k].std()\n",
    "        # find the skew\n",
    "        signature_vec[k * 5 + 3] = skew(node_features[:, k])\n",
    "        # find the kurtosis\n",
    "        signature_vec[k * 5 + 4] = kurtosis(node_features[:, k])\n",
    "\n",
    "    return signature_vec\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# sample usage\n",
    ">>>G1 = nx.karate_club_graph()\n",
    ">>>G2 = nx.krackhardt_kite_graph()\n",
    "\n",
    ">>>test = TemporalNetSimile()\n",
    ">>>print(test.dist(G1, G2))\n",
    "11.45\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function to convert data from two communities into graphs and call TemporalNetSimile\n",
    "def Compare(Community1, Community2):\n",
    "    #Convert into networkX graph to use as input for TemportalNetSimile\n",
    "    #Load Graphs\n",
    "    Graph1 = nx.Graph()\n",
    "    Graph1.add_nodes_from(Community1[\"nodes\"])\n",
    "\n",
    "    # Add edges with timestamps as edge attributes\n",
    "    for edge in Community1[\"edges\"]:\n",
    "        Graph1.add_edge(edge[\"node1\"], edge[\"node2\"], begintijd=edge[\"begintijd\"], eindtijd=edge[\"eindtijd\"])\n",
    "\n",
    "    Graph2=nx.Graph()\n",
    "    Graph2.add_nodes_from(Community2[\"nodes\"])\n",
    "    for edge in Community2[\"edges\"]:\n",
    "        Graph2.add_edge(edge[\"node1\"], edge[\"node2\"], begintijd=edge[\"begintijd\"], eindtijd=edge[\"eindtijd\"])\n",
    "\n",
    "    testTime=TemporalNetSimile()\n",
    "    Similarity = testTime.dist(Graph1, Graph2)\n",
    "    return(Similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairing evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load community datasets as rdd\n",
    "json_file_path = '../Community Detection/20Knode_20edge.json'\n",
    "with open(json_file_path, 'r') as f:\n",
    "    average_community_data = json.load(f)\n",
    "\n",
    "#community_data_with_ids = add_community_id(community_data)\n",
    "average_community_data_rdd = sc.parallelize(average_community_data)\n",
    "print(f\"number of communities: {average_community_data_rdd.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cartesian pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"number of communities: {average_community_data_rdd.count()}\")\n",
    "\n",
    "# cartesian pairs\n",
    "t0 = time.time()\n",
    "cartesian_pairs_rdd = average_community_data_rdd.cartesian(average_community_data_rdd)\n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print(f\"time: {total}, count of pairs: {cartesian_pairs_rdd.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique pairing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"number of communities: {average_community_data_rdd.count()}\")\n",
    "\n",
    "# unique pairing\n",
    "t0 = time.time()\n",
    "unique_pairs_rdd = average_community_data_rdd.cartesian(average_community_data_rdd).filter(lambda pair: str(pair[0]) < str(pair[1]))\n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print(f\"time: {total}, count of pairs: {unique_pairs_rdd.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter pairing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to filter based on nodes or edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter pair functions\n",
    "\n",
    "# group communities by number of nodes and edges\n",
    "def group_by_size(df, num_groups, column_name):\n",
    "  # sort the data by the column name\n",
    "  sorted_df = df.sort(column_name)\n",
    "\n",
    "  # calculate the size of each group\n",
    "  group_size = math.ceil(df.count() / num_groups)\n",
    "  \n",
    "  # add a row number column to the dataframe\n",
    "  df_with_row_number = df.withColumn(\"row_number\", row_number().over(Window.orderBy(column_name)))\n",
    "\n",
    "  # use row number to create groups\n",
    "  df_groups = df_with_row_number.withColumn(\"group\", ceil(col(\"row_number\") / group_size)).drop(\"row_number\")\n",
    "\n",
    "  return df_groups\n",
    "\n",
    "# make unique pairs of communities in same group\n",
    "def create_pairs_grouped_communities(df_groups):\n",
    "  df_pairs = (\n",
    "    df_groups.alias(\"df1\")\n",
    "    .join(df_groups.alias(\"df2\"), (F.col(\"df1.group\") == F.col(\"df2.group\")) & (F.col(\"df1.community_id\") < F.col(\"df2.community_id\")))\n",
    "    # .withColumn(\"pairs\", (F.col(\"df1.community_id\") + F.lit(\", \") + F.col(\"df2.community_id\")))\n",
    "    .select(\n",
    "      F.array(F.col(\"df1.community_id\"), F.col(\"df2.community_id\")).alias(\"pairs\"),\n",
    "      F.col(\"df1.group\").alias(\"group\"),\n",
    "      F.col(\"df1.community_id\").alias(\"community_id_1\"),\n",
    "      F.col(\"df2.community_id\").alias(\"community_id_2\"),\n",
    "      F.col(\"df1.nodes\").alias(\"nodes_1\"),\n",
    "      F.col(\"df2.nodes\").alias(\"nodes_2\"),\n",
    "      F.col(\"df1.edges\").alias(\"edges_1\"),\n",
    "      F.col(\"df2.edges\").alias(\"edges_2\"),\n",
    "    )\n",
    "  )\n",
    "\n",
    "  return df_pairs\n",
    "\n",
    "# complete code to turn rdd of communities into dataframe of pairs of communities of similiar node or edge size\n",
    "def rdd_community_to_dataframe_paired_community_by_column(rdd_communities, number_groups_of_communities_wanted, column_name):\n",
    "  # turn rdd into dataframe and add columns for count of nodes and edges\n",
    "  df_community = spark.createDataFrame(rdd_communities)\n",
    "  df_with_sizes = df_community \\\n",
    "    .withColumn(\"node_size\", F.size(\"nodes\")) \\\n",
    "    .withColumn(\"edge_size\", F.size(\"edges\")) \\\n",
    "\n",
    "  # group communities by number of nodes and edges\n",
    "  df_communities_groups = group_by_size(df_with_sizes, number_groups_of_communities_wanted, column_name)\n",
    "\n",
    "  # make unique pairs of communities in same group\n",
    "  result_grouped_communities = create_pairs_grouped_communities(df_communities_groups)\n",
    "\n",
    "  # from this dataframe you can see which communities are grouped together, so all the pairs as a seperate column\n",
    "\n",
    "  return result_grouped_communities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the distribution of node sizes and edge sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data of groups based on nodes and edges\n",
    "df = spark.createDataFrame(average_community_data_rdd)\n",
    "df_edge_node = df \\\n",
    "  .withColumn(\"node_size\", F.size(\"nodes\")) \\\n",
    "  .withColumn(\"edge_size\", F.size(\"edges\")) \\\n",
    "\n",
    "\n",
    "# plot distribution of node sizes and edge sizes\n",
    "df_edge_node_pd = df_edge_node.toPandas()\n",
    "plt.hist(df_edge_node_pd['node_size'], bins=10)\n",
    "plt.xlabel('Number of nodes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of node sizes of communities')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(df_edge_node_pd['edge_size'], bins=10)\n",
    "plt.xlabel('Number of edges')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of edge sizes of communities')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pair together and print the number of groups and the time it took to filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"number of communities: {average_community_data_rdd.count()}\")\n",
    "\n",
    "# filter pair\n",
    "number_of_groups_node = 2\n",
    "number_of_groups_edge = 3\n",
    "\n",
    "# nodes\n",
    "t0 = time.time()\n",
    "#Creating Datafram of grouped communities\n",
    "grouped_communities_node=rdd_community_to_dataframe_paired_community_by_column(average_community_data_rdd, number_of_groups_node, 'node_size')\n",
    "\n",
    "#Converting it to RDD\n",
    "pairs_rdd_filtering_nodes = grouped_communities_node.rdd\n",
    "t1 = time.time()\n",
    "total_node = t1-t0\n",
    "print(f\"node time: {total_node}, count of pairs: {pairs_rdd_filtering_nodes.count()}\")\n",
    "\n",
    "#edges\n",
    "t2 = time.time()\n",
    "#Creating Datafram of grouped communities\n",
    "grouped_communities_edge=rdd_community_to_dataframe_paired_community_by_column(average_community_data_rdd, number_of_groups_edge, 'edge_size')\n",
    "\n",
    "#Converting it to RDD\n",
    "pairs_rdd_filtering_edge = grouped_communities_edge.rdd\n",
    "t3 = time.time()\n",
    "total_edge = t3-t2\n",
    "print(f\"edge time: {total_edge}, count of pairs: {pairs_rdd_filtering_edge.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate similarity distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all pairs\n",
    "cartesian_pairs_rdd\n",
    "unique_pairs_rdd\n",
    "pairs_rdd_filtering_edge\n",
    "pairs_rdd_filtering_nodes\n",
    "\n",
    "def compute_similarity_alt(row):\n",
    "    # Extract community data from the row\n",
    "    community1 = {\n",
    "        'community_id': row['community_id_1'],\n",
    "        'nodes': row['nodes_1'],\n",
    "        'edges': row['edges_1']\n",
    "    }\n",
    "    community2 = {\n",
    "        'community_id': row['community_id_2'],\n",
    "        'nodes': row['nodes_2'],\n",
    "        'edges': row['edges_2']\n",
    "    }\n",
    "    group = row['group']\n",
    "\n",
    "    # Compute the distance using the Compare function\n",
    "    distance = Compare(community1, community2)\n",
    "\n",
    "    # Return a tuple: (group, community_id1, community_id2, distance)\n",
    "    return (group, community1['community_id'], community2['community_id'], distance)\n",
    "\n",
    "\n",
    "# pair similarity functions and plot distribution\n",
    "def pair_simi_plot(pairs_rdd, alt_version=False):\n",
    "  if alt_version:\n",
    "    results_rdd = pairs_rdd.map(compute_similarity_alt)\n",
    "  else:\n",
    "    #Calculate Distance\n",
    "    results_rdd = pairs_rdd.map(lambda pair: (pair, Compare(pair[0], pair[1])))\n",
    "    \n",
    "  results = results_rdd.collect()\n",
    "\n",
    "  # Collect the results into a list of tuples (community_id1, community_id2, distance)\n",
    "  similarities = []\n",
    "\n",
    "  if alt_version:\n",
    "    # temp_similarities = defaultdict(list)\n",
    "    for group, community_id1, community_id2, distance in results:\n",
    "      similarities.append((community_id1, community_id2, distance))\n",
    "      \n",
    "  else:\n",
    "    for pair, comparison_result in results:\n",
    "      community_id1 = pair[0]['community_id']\n",
    "      community_id2 = pair[1]['community_id']\n",
    "      distance = comparison_result\n",
    "      similarities.append((community_id1, community_id2, distance))\n",
    "\n",
    "  return similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract distances and time it took for each pairing method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate distances between pairing methods\n",
    "t0 = time.time()\n",
    "similarity_cartesian = pair_simi_plot(cartesian_pairs_rdd)\n",
    "t1 = time.time()\n",
    "similarity_unique = pair_simi_plot(unique_pairs_rdd)\n",
    "t2 = time.time()\n",
    "similarity_edge = pair_simi_plot(pairs_rdd_filtering_edge, True)\n",
    "t3 = time.time()\n",
    "similarity_node = pair_simi_plot(pairs_rdd_filtering_nodes, True)\n",
    "t4 = time.time()\n",
    "\n",
    "# print time each similarity calculation took on the pairs\n",
    "print(f\"cartesian time: {t1-t0}\")\n",
    "print(f\"unique time: {t2-t1}\")\n",
    "print(f\"edge time: {t3-t2}\")\n",
    "print(f\"node time: {t4-t3}\")\n",
    "\n",
    "# Extract distances for analysis\n",
    "distances_cartesian = [distance for _, _, distance in similarity_cartesian]\n",
    "distances_unique = [distance for _, _, distance in similarity_unique]\n",
    "distances_edge = [distance for _, _, distance in similarity_edge]\n",
    "distances_node = [distance for _, _, distance in similarity_node]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize in graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distribution of similarities in same graph\n",
    "plt.hist(distances_cartesian, bins=100, histtype='step', linewidth=2, label='cartesian', color='#1f77b4')\n",
    "plt.hist(distances_unique, bins=100, histtype='step', linewidth=2, label='unique', color='#d62728')\n",
    "plt.hist(distances_edge, bins=10, histtype='step', linewidth=2, label='edge', color='#2ca02c')\n",
    "plt.hist(distances_node, bins=10, histtype='step', linewidth=2, label='node', color='#ff7f0e')\n",
    "plt.xlabel('Similarity distance')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(np.arange(0, 25, step=2))\n",
    "plt.yticks(np.arange(0, 10, step=1))\n",
    "plt.title('Histogram of Distances Between Communities, with different pairings')\n",
    "plt.legend(loc='upper right')\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Distance\n",
    "results_rdd = unique_pairs_rdd.map(lambda pair: (pair, Compare(pair[0], pair[1])))\n",
    "\n",
    "#Print\n",
    "results = results_rdd.collect()\n",
    "\n",
    "# Collect the results into a list of tuples (community_id1, community_id2, distance)\n",
    "similarities = []\n",
    "\n",
    "for pair, comparison_result in results:\n",
    "    community_id1 = pair[0]['community_id']\n",
    "    community_id2 = pair[1]['community_id']\n",
    "    distance = comparison_result\n",
    "    similarities.append((community_id1, community_id2, distance))\n",
    "\n",
    "# Optionally, print the collected similarities\n",
    "# print(\"Collected Similarities:\")\n",
    "# for sim in similarities:\n",
    "#     print(f\"Community {sim[0]} and Community {sim[1]}: Distance = {sim[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract distances for analysis\n",
    "distances = [distance for _, _, distance in similarities]\n",
    "\n",
    "# Plot histogram of distances\n",
    "plt.hist(distances, bins=30)\n",
    "plt.xlabel('Distance')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Distances Between Communities')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and standard deviation\n",
    "mean_distance = np.mean(distances)\n",
    "std_distance = np.std(distances)\n",
    "\n",
    "# Set threshold at one standard deviation below the mean\n",
    "threshold = mean_distance - std_distance\n",
    "print(threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform hierarchical clustering with average linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_ids = sorted(set([id for sim in similarities for id in sim[:2]]))\n",
    "id_to_index = {id: idx for idx, id in enumerate(community_ids)}\n",
    "N = len(community_ids)\n",
    "\n",
    "# Initialize a full distance matrix with zeros\n",
    "distance_matrix = np.zeros((N, N))\n",
    "\n",
    "# Fill in the distances\n",
    "for community_id1, community_id2, distance in similarities:\n",
    "    i, j = id_to_index[community_id1], id_to_index[community_id2]\n",
    "    distance_matrix[i, j] = distance\n",
    "    distance_matrix[j, i] = distance  # Symmetric matrix\n",
    "\n",
    "# Convert to condensed distance matrix required for linkage\n",
    "condensed_dists = squareform(distance_matrix)\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "Z = linkage(condensed_dists, method='average')\n",
    "\n",
    "# Plot the dendrogram (optional)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(Z, labels=community_ids)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Community ID')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n",
    "\n",
    "# Choose a threshold based on the dendrogram\n",
    "threshold = 8.5  # Adjust as necessary\n",
    "\n",
    "# Get cluster labels\n",
    "cluster_labels = fcluster(Z, t=threshold, criterion='distance')\n",
    "\n",
    "# Group communities by cluster labels\n",
    "groups = defaultdict(list)\n",
    "for community_id, cluster_label in zip(community_ids, cluster_labels):\n",
    "    groups[cluster_label].append(community_id)\n",
    "\n",
    "# Output the groups\n",
    "print(\"\\nGroups of Similar Communities (Hierarchical Clustering):\")\n",
    "for cluster_label, community_list in groups.items():\n",
    "    print(f\"Group {cluster_label}: {sorted(community_list)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(distance_matrix)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(distance_matrix, xticklabels=community_ids, yticklabels=community_ids, cmap='viridis')\n",
    "plt.title('Heatmap of Community Distances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print statistical measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_distance = np.min(distances)\n",
    "max_distance = np.max(distances)\n",
    "mean_distance = np.mean(distances)\n",
    "median_distance = np.median(distances)\n",
    "print(f\"Min Distance: {min_distance}\")\n",
    "print(f\"Max Distance: {max_distance}\")\n",
    "print(f\"Mean Distance: {mean_distance}\")\n",
    "print(f\"Median Distance: {median_distance}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print dendogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "dendrogram(Z, labels=community_ids, leaf_rotation=90)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Community ID')\n",
    "plt.ylabel('Distance')\n",
    "plt.axhline(y=threshold, c='k', ls='--', lw=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate linkage methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage_methods = ['single', 'complete', 'average', 'ward']\n",
    "for method in linkage_methods:\n",
    "    Z = linkage(condensed_dists, method=method)\n",
    "    cluster_labels = fcluster(Z, t=threshold, criterion='distance')\n",
    "\n",
    "    # Group communities\n",
    "    groups = defaultdict(list)\n",
    "    for community_id, cluster_label in zip(community_ids, cluster_labels):\n",
    "        groups[cluster_label].append(community_id)\n",
    "\n",
    "    # Output the groups for each method\n",
    "    print(f\"\\nLinkage Method: {method}\")\n",
    "    for cluster_label, community_list in groups.items():\n",
    "        print(f\"Group {cluster_label}: {sorted(community_list)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
